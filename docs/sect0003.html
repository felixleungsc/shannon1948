<!DOCTYPE html>
<html lang="en">
<head>
<script>
  MathJax = { 
    tex: {
		    inlineMath: [['\\(','\\)']]
	} }
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<meta name="generator" content="plasTeX" />
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>A Mathematical Theory of Communication: The Discrete Source of Information</title>
<link rel="next" href="sect0004.html" title="The Series of Approximations to English" />
<link rel="prev" href="sec-1.html" title="The Discrete Noiseless Channel" />
<link rel="up" href="sect0002.html" title="Discrete Noiseless Systems" />
<link rel="stylesheet" href="styles/theme-white.css" />
<link rel="stylesheet" href="styles/amsthm.css" />
</head>

<body>
<header>
<svg  id="toc-toggle" class="icon icon-list-numbered "><use xlink:href="symbol-defs.svg#icon-list-numbered"></use></svg>
<h1 id="doc_title"><a href="index.html">A Mathematical Theory of Communication</a></h1>
</header>

<div class="wrapper">
<nav class="toc">
<ul class="sub-toc-0">
<li class="">
  <a href="sect0001.html"><span class="toc_ref"></span> <span class="toc_entry">Introduction</span></a>
 </li>
<li class=" active">
  <a href="sect0002.html"><span class="toc_ref">1</span> <span class="toc_entry">Discrete Noiseless Systems</span></a>
  <span class="expand-toc">▼</span>
  <ul class="sub-toc-1 active">
     <li class="">
  <a href="sec-1.html"><span class="toc_ref">1</span> <span class="toc_entry">The Discrete Noiseless Channel</span></a>
 </li>
<li class=" active current">
  <a href="sect0003.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Source of Information</span></a>
 </li>
<li class="">
  <a href="sect0004.html"><span class="toc_ref">3</span> <span class="toc_entry">The Series of Approximations to English</span></a>
 </li>
<li class="">
  <a href="sect0005.html"><span class="toc_ref">4</span> <span class="toc_entry">Graphical Representation of a Markoff Process</span></a>
 </li>
<li class="">
  <a href="sect0006.html"><span class="toc_ref">5</span> <span class="toc_entry">Ergodic and Mixed Sources</span></a>
 </li>
<li class="">
  <a href="sect0007.html"><span class="toc_ref">6</span> <span class="toc_entry">Choice, Uncertainty and Entropy</span></a>
 </li>
<li class="">
  <a href="sect0008.html"><span class="toc_ref">7</span> <span class="toc_entry">The Entropy of an Information Source</span></a>
 </li>
<li class="">
  <a href="sect0009.html"><span class="toc_ref">8</span> <span class="toc_entry">Representation of the Encoding and Decoding Operations</span></a>
 </li>
<li class="">
  <a href="sect0010.html"><span class="toc_ref">9</span> <span class="toc_entry">The Fundamental Theorem for a Noiseless Channel</span></a>
 </li>
<li class="">
  <a href="sect0011.html"><span class="toc_ref">10</span> <span class="toc_entry">Discussion and Examples</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0012.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Channel with Noise</span></a>
  <span class="expand-toc">▶</span>
  <ul class="sub-toc-1">
     <li class="">
  <a href="sect0013.html"><span class="toc_ref">11</span> <span class="toc_entry">Representation of a Noisy Discrete Channel</span></a>
 </li>
<li class="">
  <a href="sect0014.html"><span class="toc_ref">12</span> <span class="toc_entry">Equivocation and Channel Capacity</span></a>
 </li>
<li class="">
  <a href="sect0015.html"><span class="toc_ref">13</span> <span class="toc_entry">The Fundamental Theorem for a Discrete Channel with Noise</span></a>
 </li>
<li class="">
  <a href="sect0016.html"><span class="toc_ref">14</span> <span class="toc_entry">Discussion</span></a>
 </li>
<li class="">
  <a href="sect0017.html"><span class="toc_ref">15</span> <span class="toc_entry">Example of a Discrete Channel and its Capacity</span></a>
 </li>
<li class="">
  <a href="sect0018.html"><span class="toc_ref">16</span> <span class="toc_entry">The Channel Capacity in Certain Special Cases</span></a>
 </li>
<li class="">
  <a href="sect0019.html"><span class="toc_ref">17</span> <span class="toc_entry">An Example of Efficient Coding</span></a>
 </li>
<li class="">
  <a href="ap-1.html"><span class="toc_ref">A</span> <span class="toc_entry">The Growth of the Number of Blocks of Symbols with a Finite State Condition</span></a>
 </li>
<li class="">
  <a href="ap-2.html"><span class="toc_ref">B</span> <span class="toc_entry">Derivation of \(H=-\sum p_i\log p_i\)</span></a>
 </li>
<li class="">
  <a href="ap-3.html"><span class="toc_ref">C</span> <span class="toc_entry">Theorems on Ergodic Sources</span></a>
 </li>
<li class="">
  <a href="ap-4.html"><span class="toc_ref">D</span> <span class="toc_entry">Maximizing the Rate for a System of Constraints</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0020.html"><span class="toc_ref">3</span> <span class="toc_entry">Mathematical Preliminaries</span></a>
 </li>
<li class="">
  <a href="sect0021.html"><span class="toc_ref">4</span> <span class="toc_entry">The Continuous Channel</span></a>
 </li>
<li class="">
  <a href="sect0022.html"><span class="toc_ref">5</span> <span class="toc_entry">The Rate for a Continuous Source</span></a>
 </li>
</ul>
</nav>

<div class="content">
<div class="content-wrapper">


<div class="main-text">
<h1 id="a0000000004">2 The Discrete Source of Information</h1>
<p>We have seen that under very general conditions the logarithm of the number of possible signals in a discrete channel increases linearly with time. The capacity to transmit information can be specified by giving this rate of increase, the number of bits per second required to specify the particular signal used. </p>
<p>We now consider the information source. How is an information source to be described mathematically, and how much information in bits per second is produced in a given source? The main point at issue is the effect of statistical knowledge about the source in reducing the required capacity of the channel, by the use of proper encoding of the information. In telegraphy, for example, the messages to be transmitted consist of sequences of letters. These sequences, however, are not completely random. In general, they form sentences and have the statistical structure of, say, English. The letter E occurs more frequently than Q, the sequence TH more frequently than XP, etc. The existence of this structure allows one to make a saving in time (or channel capacity) by properly encoding the message sequences into signal sequences. This is already done to a limited extent in telegraphy by using the shortest channel symbol, a dot, for the most common English letter E; while the infrequent letters, Q, X, Z are represented by longer sequences of dots and dashes. This idea is carried still further in certain commercial codes where common words and phrases are represented by four- or five-letter code groups with a considerable saving in average time. The standardized greeting and anniversary telegrams now in use extend this to the point of encoding a sentence or two into a relatively short sequence of numbers. </p>
<p>We can think of a discrete source as generating the message, symbol by symbol. It will choose successive symbols according to certain probabilities depending, in general, on preceding choices as well as the particular symbols in question. A physical system, or a mathematical model of a system which produces such a sequence of symbols governed by a set of probabilities, is known as a stochastic process.<a class="footnote" href="#a0000000034">
  <sup class="footnotemark">1</sup>
</a> We may consider a discrete source, therefore, to be represented by a stochastic process. Conversely, any stochastic process which produces a discrete sequence of symbols chosen from a finite set may be considered a discrete source. This will include such cases as: </p>
<ol class="enumerate">
  <li><p>Natural written languages such as English, German, Chinese. </p>
</li>
  <li><p>Continuous information sources that have been rendered discrete by some quantizing process. For example, the quantized speech from a PCM transmitter, or a quantized television signal. </p>
</li>
  <li><p>Mathematical cases where we merely define abstractly a stochastic process which generates a sequence of symbols. The following are examples of this last type of source. </p>
<ol class="enumerate">
  <li></li>
  <li> </li>
  <li><p>Suppose we have five letters A, B, C, D, E which are chosen each with probability .2, successive choices being independent. This would lead to a sequence of which the following is a typical example. </p>
<p>B D C B C E C C C A D C B D D A A E C E E A<br />A B B D A E E C A C E E B A E E C B C E A D. </p>
<p>This was constructed with the use of a table of random numbers.<a class="footnote" href="#a0000000035">
  <sup class="footnotemark">2</sup>
</a> </p>
</li>
  <li><p>Using the same five letters let the probabilities be .4, .1, .2, .2, .1, respectively, with successive choices independent. A typical message from this source is then: </p>
<p>A A A C D C B D C E A A D A D A C E D A<br />E A D C A B E D A D D C E C A A A A A D. </p>
</li>
  <li><p>A more complicated structure is obtained if successive symbols are not chosen independently but their probabilities depend on preceding letters. In the simplest case of this type a choice depends only on the preceding letter and not on ones before that. The statistical structure can then be described by a set of transition probabilities \(p_i(j)\), the probability that letter \(i\) is followed by letter \(j\). The indices \(i\) and \(j\) range over all the possible symbols. A second equivalent way of specifying the structure is to give the “digram” probabilities \(p(i,j)\), i.e., the relative frequency of the digram \(i ~ j\). The letter frequencies \(p(i)\), (the probability of letter \(i\)), the transition probabilities \(p_i(j)\) and the digram probabilities \(p(i,j)\) are related by the following formulas: </p>
<div class="displaymath" id="a0000000036">
  \begin{align*}  p(i)& = \sum _j p(i,j) = \sum _j p(j,i) = \sum _j p(j) p_j (i) \\ p(i,j)& = p(i) p_i(j) \\ \sum _j^{\rule{0pt}{1ex}} p_i (j)& = \sum _i p(i) = \sum _{i,j} p(i,j) = 1. \end{align*}
</div>
<p>As a specific example suppose there are three letters A, B, C with the probability tables: </p>
<div class="displaymath" id="a0000000037">
  \[  \renewcommand{\arraystretch }{1.2} \begin{array}{r | c c c} \multicolumn{1}{c}{p_i(j)} &  \multicolumn{3}{|c}{j} \\ &  \text{A} &  \text{B} &  \text{C} \\ \hline \text{A} &  0 &  \frac45 &  \frac15 \\ i\quad \text{B} &  \frac12 &  \frac12 &  0 \\ \text{C} &  \frac12 &  \frac25 &  \frac{1}{10} \end{array} \qquad \begin{array}{c|c} i &  p(i) \\ \text{A} &  \frac{9}{27} \\ \text{B} &  \frac{16}{27} \\ \text{C} &  \frac{2}{27} \end{array} \qquad \begin{array}{r | c c c} \multicolumn{1}{c}{p(i,j)} &  \multicolumn{3}{|c}{j} \\ &  \text{A} &  \text{B} &  \text{C} \\ \hline \text{A} &  0 &  \frac{4}{15} &  \frac{1}{15} \\ i\quad \text{B} &  \frac{8}{27} &  \frac{8}{27} &  0 \\ \text{C} &  \frac{1}{27} &  \frac{4}{135} &  \frac{1}{135} \end{array}  \]
</div>
<p> A typical message from this source is the following: </p>
<p>A B B A B A B A B A B A B A B B B A B B B B B A B A B A B A B A B B B A C A C A B B A B B B B A B B A B A C B B B A B A. </p>
<p>The next increase in complexity would involve trigram frequencies but no more. The choice of a letter would depend on the preceding two letters but not on the message before that point. A set of trigram frequencies \(p(i,j,k)\) or equivalently a set of transition probabilities \(p_{ij}(k)\) would be required. Continuing in this way one obtains successively more complicated stochastic processes. In the general \(n\)-gram case a set of \(n\)-gram probabilities \(p(i_1,i_2,\dots ,i_n)\) or of transition probabilities \(p_{i_1,i_2,\dots ,i_{n-1}}(i_n)\) is required to specify the statistical structure. </p>
</li>
  <li><p>Stochastic processes can also be defined which produce a text consisting of a sequence of “words.” Suppose there are five letters A, B, C, D, E and 16 “words” in the language with associated probabilities: </p>
<div class="centered"> <table class="tabular">
  <tr>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .10 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> A </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .16 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> BEBE </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .11 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> CABED </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .04 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> DEB </p>

    </td>
  </tr>
  <tr>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p>.04 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> ADEB </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .04 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> BED </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .05 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> CEED </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .15 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> DEED </p>

    </td>
  </tr>
  <tr>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p>.05 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> ADEE </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .02 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> BEED </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .08 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> DAB </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .01 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> EAB </p>

    </td>
  </tr>
  <tr>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p>.01 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> BADD </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .05 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> CA </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .04 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> DAD </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> .05 </p>

    </td>
    <td  style="text-align:left" 
        rowspan=""
        colspan="">
      <p> EE </p>

    </td>
  </tr>
</table> </div>
<p> Suppose successive “words” are chosen independently and are separated by a space. A typical message might be: </p>
<p>DAB EE A BEBE DEED DEB ADEE ADEE EE DEB BEBE BEBE BEBE ADEE BED DEED DEED CEED ADEE A DEED DEED BEBE CABED BEBE BED DAB DEED ADEB. </p>
<p>If all the words are of finite length this process is equivalent to one of the preceding type, but the description may be simpler in terms of the word structure and probabilities. We may also generalize here and introduce transition probabilities between words, etc. </p>
</li>
</ol>
</li>
</ol>
<p>These artificial languages are useful in constructing simple problems and examples to illustrate various possibilities. We can also approximate to a natural language by means of a series of simple artificial languages. The zero-order approximation is obtained by choosing all letters with the same probability and independently. The first-order approximation is obtained by choosing successive letters independently but each letter having the same probability that it has in the natural language.<a class="footnote" href="#a0000000038">
  <sup class="footnotemark">3</sup>
</a> Thus, in the first-order approximation to English, E is chosen with probability .12 (its frequency in normal English) and W with probability&#160;.02, but there is no influence between adjacent letters and no tendency to form the preferred digrams such as TH, ED, etc. In the second-order approximation, digram structure is introduced. After a letter is chosen, the next one is chosen in accordance with the frequencies with which the various letters follow the first one. This requires a table of digram frequencies \(p_i(j)\). In the third-order approximation, trigram structure is introduced. Each letter is chosen with probabilities which depend on the preceding two letters. </p>

</div> <!--main-text -->
<footer id="footnotes">
<ol>
  <li id="a0000000034">See, for example, S. Chandrasekhar, “Stochastic Problems in Physics and Astronomy,” <i class="it">Reviews of Modern Physics</i>, v.&#160;15, No.&#160;1, January 1943, p.&#160;1.</li>
  <li id="a0000000035">Kendall and Smith, <i class="it">Tables of Random Sampling Numbers,</i> Cambridge, 1939.</li>
  <li id="a0000000038">Letter, digram and trigram frequencies are given in <i class="it">Secret and Urgent </i> by Fletcher Pratt, Blue Ribbon Books, 1939. Word frequencies are tabulated in <i class="it">Relative Frequency of English Speech Sounds,</i> G. Dewey, Harvard University Press, 1923.</li>
</ol>
</footer>
</div> <!-- content-wrapper -->
</div> <!-- content -->
</div> <!-- wrapper -->

<nav class="prev_up_next">
  <a href="sec-1.html" title="The Discrete Noiseless Channel"><svg  class="icon icon-arrow-left "><use xlink:href="symbol-defs.svg#icon-arrow-left"></use></svg>
</a>
  <a href="sect0002.html" title="Discrete Noiseless Systems"><svg  class="icon icon-arrow-up "><use xlink:href="symbol-defs.svg#icon-arrow-up"></use></svg>
</a>
  <a href="sect0004.html" title="The Series of Approximations to English"><svg  class="icon icon-arrow-right "><use xlink:href="symbol-defs.svg#icon-arrow-right"></use></svg>
</a>
</nav>

<script type="text/javascript" src="js/jquery.min.js"></script>
<script type="text/javascript" src="js/plastex.js"></script>
<script type="text/javascript" src="js/svgxuse.js"></script>
</body>
</html>