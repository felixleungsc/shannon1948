<!DOCTYPE html>
<html lang="en">
<head>
<script>
  MathJax = { 
    tex: {
		    inlineMath: [['\\(','\\)']]
	} }
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<meta name="generator" content="plasTeX" />
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>A Mathematical Theory of Communication: Representation of the Encoding and Decoding Operations</title>
<link rel="next" href="sect0010.html" title="The Fundamental Theorem for a Noiseless Channel" />
<link rel="prev" href="sect0008.html" title="The Entropy of an Information Source" />
<link rel="up" href="sect0002.html" title="Discrete Noiseless Systems" />
<link rel="stylesheet" href="styles/theme-white.css" />
<link rel="stylesheet" href="styles/amsthm.css" />
</head>

<body>
<header>
<svg  id="toc-toggle" class="icon icon-list-numbered "><use xlink:href="symbol-defs.svg#icon-list-numbered"></use></svg>
<h1 id="doc_title"><a href="index.html">A Mathematical Theory of Communication</a></h1>
</header>

<div class="wrapper">
<nav class="toc">
<ul class="sub-toc-0">
<li class="">
  <a href="sect0001.html"><span class="toc_ref"></span> <span class="toc_entry">Introduction</span></a>
 </li>
<li class=" active">
  <a href="sect0002.html"><span class="toc_ref">1</span> <span class="toc_entry">Discrete Noiseless Systems</span></a>
  <span class="expand-toc">▼</span>
  <ul class="sub-toc-1 active">
     <li class="">
  <a href="sec-1.html"><span class="toc_ref">1</span> <span class="toc_entry">The Discrete Noiseless Channel</span></a>
 </li>
<li class="">
  <a href="sect0003.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Source of Information</span></a>
 </li>
<li class="">
  <a href="sect0004.html"><span class="toc_ref">3</span> <span class="toc_entry">The Series of Approximations to English</span></a>
 </li>
<li class="">
  <a href="sect0005.html"><span class="toc_ref">4</span> <span class="toc_entry">Graphical Representation of a Markoff Process</span></a>
 </li>
<li class="">
  <a href="sect0006.html"><span class="toc_ref">5</span> <span class="toc_entry">Ergodic and Mixed Sources</span></a>
 </li>
<li class="">
  <a href="sect0007.html"><span class="toc_ref">6</span> <span class="toc_entry">Choice, Uncertainty and Entropy</span></a>
 </li>
<li class="">
  <a href="sect0008.html"><span class="toc_ref">7</span> <span class="toc_entry">The Entropy of an Information Source</span></a>
 </li>
<li class=" active current">
  <a href="sect0009.html"><span class="toc_ref">8</span> <span class="toc_entry">Representation of the Encoding and Decoding Operations</span></a>
 </li>
<li class="">
  <a href="sect0010.html"><span class="toc_ref">9</span> <span class="toc_entry">The Fundamental Theorem for a Noiseless Channel</span></a>
 </li>
<li class="">
  <a href="sect0011.html"><span class="toc_ref">10</span> <span class="toc_entry">Discussion and Examples</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0012.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Channel with Noise</span></a>
  <span class="expand-toc">▶</span>
  <ul class="sub-toc-1">
     <li class="">
  <a href="sect0013.html"><span class="toc_ref">11</span> <span class="toc_entry">Representation of a Noisy Discrete Channel</span></a>
 </li>
<li class="">
  <a href="sect0014.html"><span class="toc_ref">12</span> <span class="toc_entry">Equivocation and Channel Capacity</span></a>
 </li>
<li class="">
  <a href="sect0015.html"><span class="toc_ref">13</span> <span class="toc_entry">The Fundamental Theorem for a Discrete Channel with Noise</span></a>
 </li>
<li class="">
  <a href="sect0016.html"><span class="toc_ref">14</span> <span class="toc_entry">Discussion</span></a>
 </li>
<li class="">
  <a href="sect0017.html"><span class="toc_ref">15</span> <span class="toc_entry">Example of a Discrete Channel and its Capacity</span></a>
 </li>
<li class="">
  <a href="sect0018.html"><span class="toc_ref">16</span> <span class="toc_entry">The Channel Capacity in Certain Special Cases</span></a>
 </li>
<li class="">
  <a href="sect0019.html"><span class="toc_ref">17</span> <span class="toc_entry">An Example of Efficient Coding</span></a>
 </li>
<li class="">
  <a href="ap-1.html"><span class="toc_ref">A</span> <span class="toc_entry">The Growth of the Number of Blocks of Symbols with a Finite State Condition</span></a>
 </li>
<li class="">
  <a href="ap-2.html"><span class="toc_ref">B</span> <span class="toc_entry">Derivation of \(H=-\sum p_i\log p_i\)</span></a>
 </li>
<li class="">
  <a href="ap-3.html"><span class="toc_ref">C</span> <span class="toc_entry">Theorems on Ergodic Sources</span></a>
 </li>
<li class="">
  <a href="ap-4.html"><span class="toc_ref">D</span> <span class="toc_entry">Maximizing the Rate for a System of Constraints</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0020.html"><span class="toc_ref">3</span> <span class="toc_entry">Mathematical Preliminaries</span></a>
 </li>
<li class="">
  <a href="sect0021.html"><span class="toc_ref">4</span> <span class="toc_entry">The Continuous Channel</span></a>
 </li>
<li class="">
  <a href="sect0022.html"><span class="toc_ref">5</span> <span class="toc_entry">The Rate for a Continuous Source</span></a>
 </li>
</ul>
</nav>

<div class="content">
<div class="content-wrapper">


<div class="main-text">
<h1 id="a0000000010">8 Representation of the Encoding and Decoding Operations</h1>
<p>We have yet to represent mathematically the operations performed by the transmitter and receiver in encoding and decoding the information. Either of these will be called a discrete transducer. The input to the transducer is a sequence of input symbols and its output a sequence of output symbols. The transducer may have an internal memory so that its output depends not only on the present input symbol but also on the past history. We assume that the internal memory is finite, i.e., there exist a finite number \(m\) of possible states of the transducer and that its output is a function of the present state and the present input symbol. The next state will be a second function of these two quantities. Thus a transducer can be described by two functions: </p>
<div class="displaymath" id="a0000000070">
  \begin{align*}  y_n & = f(x_n, \alpha _n) \\ \alpha _{n+1} & = g(x_n, \alpha _n) \end{align*}
</div>
<p> where </p>
<dl class="description">
  <dt>\(x_n\)</dt>
  <dd><p>is the \(n^{\text{th}}\) input symbol, </p>
</dd>
  <dt>\(\alpha _n\)</dt>
  <dd><p>is the state of the transducer when the \(n^{\text{th}}\) input symbol is introduced, </p>
</dd>
  <dt>\(y_n\)</dt>
  <dd><p>is the output symbol (or sequence of output symbols) produced when \(x_n\) is introduced if the state is \(\alpha _n\). </p>
</dd>
</dl>
<p>If the output symbols of one transducer can be identified with the input symbols of a second, they can be connected in tandem and the result is also a transducer. If there exists a second transducer which operates on the output of the first and recovers the original input, the first transducer will be called non-singular and the second will be called its inverse. </p>
<div class="theorem_thmwrapper theorem-style-plain" id="thm:7">
  <div class="theorem_thmheading">
    <span class="theorem_thmcaption">
    Theorem
    </span>
    <span class="theorem_thmlabel">7</span>
  </div>
  <div class="theorem_thmcontent">
  <p> The output of a finite state transducer driven by a finite state statistical source is a finite state statistical source, with entropy (per unit time) less than or equal to that of the input. If the transducer is non-singular they are equal. </p>

  </div>
</div>
<p>Let \(\alpha \) represent the state of the source, which produces a sequence of symbols \(x_i\); and let \(\beta \) be the state of the transducer, which produces, in its output, blocks of symbols \(y_j\). The combined system can be represented by the “product state space” of pairs \((\alpha ,\beta )\). Two points in the space \((\alpha _1,\beta _1 )\) and \((\alpha _2,\beta _2 )\), are connected by a line if \(\alpha _1\) can produce an \(x\) which changes \(\beta _1\) to \(\beta _2\), and this line is given the probability of that \(x\) in this case. The line is labeled with the block of \(y_j\) symbols produced by the transducer. The entropy of the output can be calculated as the weighted sum over the states. If we sum first on \(\beta \) each resulting term is less than or equal to the corresponding term for \(\alpha \), hence the entropy is not increased. If the transducer is non-singular let its output be connected to the inverse transducer. If \(H'_1\), \(H'_2\) and \(H'_3\) are the output entropies of the source, the first and second transducers respectively, then \(H'_1 \ge H'_2 \ge H'_3 = H'_1\) and therefore \(H'_1 = H'_2\). </p>
<p>Suppose we have a system of constraints on possible sequences of the type which can be represented by a linear graph as in Fig.&#160;<a href="sec-1.html#fig:2">2</a>. If probabilities \(p_{ij}^{(s)}\) were assigned to the various lines connecting state \(i\) to state \(j\) this would become a source. There is one particular assignment which maximizes the resulting entropy (see Appendix&#160;<a href="ap-4.html">D</a>). </p>
<div class="theorem_thmwrapper theorem-style-plain" id="thm:8">
  <div class="theorem_thmheading">
    <span class="theorem_thmcaption">
    Theorem
    </span>
    <span class="theorem_thmlabel">8</span>
  </div>
  <div class="theorem_thmcontent">
  <p> Let the system of constraints considered as a channel have a capacity \(C=\log W\). If we assign </p>
<div class="displaymath" id="a0000000071">
  \[  p_{ij}^{(s)} = \frac{B_j}{B_i} W^{-\ell _{ij}^{(s)}}  \]
</div>
<p> where \(\ell _{ij}^{(s)}\) is the duration of the \(s^{\text{th}}\) symbol leading from state \(i\) to state \(j\) and the \(B_i\) satisfy </p>
<div class="displaymath" id="a0000000072">
  \[  B_i = \sum _{s,j} B_j W^{-\ell _{ij}^{(s)}}  \]
</div>
<p> then \(H\) is maximized and equal to \(C\). </p>

  </div>
</div>
<p>By proper assignment of the transition probabilities the entropy of symbols on a channel can be maximized at the channel capacity. </p>

</div> <!--main-text -->
</div> <!-- content-wrapper -->
</div> <!-- content -->
</div> <!-- wrapper -->

<nav class="prev_up_next">
  <a href="sect0008.html" title="The Entropy of an Information Source"><svg  class="icon icon-arrow-left "><use xlink:href="symbol-defs.svg#icon-arrow-left"></use></svg>
</a>
  <a href="sect0002.html" title="Discrete Noiseless Systems"><svg  class="icon icon-arrow-up "><use xlink:href="symbol-defs.svg#icon-arrow-up"></use></svg>
</a>
  <a href="sect0010.html" title="The Fundamental Theorem for a Noiseless Channel"><svg  class="icon icon-arrow-right "><use xlink:href="symbol-defs.svg#icon-arrow-right"></use></svg>
</a>
</nav>

<script type="text/javascript" src="js/jquery.min.js"></script>
<script type="text/javascript" src="js/plastex.js"></script>
<script type="text/javascript" src="js/svgxuse.js"></script>
</body>
</html>