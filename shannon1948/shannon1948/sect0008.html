<!DOCTYPE html>
<html lang="en">
<head>
<script>
  MathJax = { 
    tex: {
		    inlineMath: [['\\(','\\)']]
	} }
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<meta name="generator" content="plasTeX" />
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>A Mathematical Theory of Communication: The Entropy of an Information Source</title>
<link rel="next" href="sect0009.html" title="Representation of the Encoding and Decoding Operations" />
<link rel="prev" href="sect0007.html" title="Choice, Uncertainty and Entropy" />
<link rel="up" href="sect0002.html" title="Discrete Noiseless Systems" />
<link rel="stylesheet" href="styles/theme-white.css" />
<link rel="stylesheet" href="styles/amsthm.css" />
</head>

<body>
<header>
<svg  id="toc-toggle" class="icon icon-list-numbered "><use xlink:href="symbol-defs.svg#icon-list-numbered"></use></svg>
<h1 id="doc_title"><a href="index.html">A Mathematical Theory of Communication</a></h1>
</header>

<div class="wrapper">
<nav class="toc">
<ul class="sub-toc-0">
<li class="">
  <a href="sect0001.html"><span class="toc_ref"></span> <span class="toc_entry">Introduction</span></a>
 </li>
<li class=" active">
  <a href="sect0002.html"><span class="toc_ref">1</span> <span class="toc_entry">Discrete Noiseless Systems</span></a>
  <span class="expand-toc">▼</span>
  <ul class="sub-toc-1 active">
     <li class="">
  <a href="sec-1.html"><span class="toc_ref">1</span> <span class="toc_entry">The Discrete Noiseless Channel</span></a>
 </li>
<li class="">
  <a href="sect0003.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Source of Information</span></a>
 </li>
<li class="">
  <a href="sect0004.html"><span class="toc_ref">3</span> <span class="toc_entry">The Series of Approximations to English</span></a>
 </li>
<li class="">
  <a href="sect0005.html"><span class="toc_ref">4</span> <span class="toc_entry">Graphical Representation of a Markoff Process</span></a>
 </li>
<li class="">
  <a href="sect0006.html"><span class="toc_ref">5</span> <span class="toc_entry">Ergodic and Mixed Sources</span></a>
 </li>
<li class="">
  <a href="sect0007.html"><span class="toc_ref">6</span> <span class="toc_entry">Choice, Uncertainty and Entropy</span></a>
 </li>
<li class=" active current">
  <a href="sect0008.html"><span class="toc_ref">7</span> <span class="toc_entry">The Entropy of an Information Source</span></a>
 </li>
<li class="">
  <a href="sect0009.html"><span class="toc_ref">8</span> <span class="toc_entry">Representation of the Encoding and Decoding Operations</span></a>
 </li>
<li class="">
  <a href="sect0010.html"><span class="toc_ref">9</span> <span class="toc_entry">The Fundamental Theorem for a Noiseless Channel</span></a>
 </li>
<li class="">
  <a href="sect0011.html"><span class="toc_ref">10</span> <span class="toc_entry">Discussion and Examples</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0012.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Channel with Noise</span></a>
  <span class="expand-toc">▶</span>
  <ul class="sub-toc-1">
     <li class="">
  <a href="sect0013.html"><span class="toc_ref">11</span> <span class="toc_entry">Representation of a Noisy Discrete Channel</span></a>
 </li>
<li class="">
  <a href="sect0014.html"><span class="toc_ref">12</span> <span class="toc_entry">Equivocation and Channel Capacity</span></a>
 </li>
<li class="">
  <a href="sect0015.html"><span class="toc_ref">13</span> <span class="toc_entry">The Fundamental Theorem for a Discrete Channel with Noise</span></a>
 </li>
<li class="">
  <a href="sect0016.html"><span class="toc_ref">14</span> <span class="toc_entry">Discussion</span></a>
 </li>
<li class="">
  <a href="sect0017.html"><span class="toc_ref">15</span> <span class="toc_entry">Example of a Discrete Channel and its Capacity</span></a>
 </li>
<li class="">
  <a href="sect0018.html"><span class="toc_ref">16</span> <span class="toc_entry">The Channel Capacity in Certain Special Cases</span></a>
 </li>
<li class="">
  <a href="sect0019.html"><span class="toc_ref">17</span> <span class="toc_entry">An Example of Efficient Coding</span></a>
 </li>
<li class="">
  <a href="ap-1.html"><span class="toc_ref">A</span> <span class="toc_entry">The Growth of the Number of Blocks of Symbols with a Finite State Condition</span></a>
 </li>
<li class="">
  <a href="ap-2.html"><span class="toc_ref">B</span> <span class="toc_entry">Derivation of \(H=-\sum p_i\log p_i\)</span></a>
 </li>
<li class="">
  <a href="ap-3.html"><span class="toc_ref">C</span> <span class="toc_entry">Theorems on Ergodic Sources</span></a>
 </li>
<li class="">
  <a href="ap-4.html"><span class="toc_ref">D</span> <span class="toc_entry">Maximizing the Rate for a System of Constraints</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0020.html"><span class="toc_ref">3</span> <span class="toc_entry">Mathematical Preliminaries</span></a>
 </li>
<li class="">
  <a href="sect0021.html"><span class="toc_ref">4</span> <span class="toc_entry">The Continuous Channel</span></a>
 </li>
<li class="">
  <a href="sect0022.html"><span class="toc_ref">5</span> <span class="toc_entry">The Rate for a Continuous Source</span></a>
 </li>
</ul>
</nav>

<div class="content">
<div class="content-wrapper">


<div class="main-text">
<h1 id="a0000000009">7 The Entropy of an Information Source</h1>
<p>Consider a discrete source of the finite state type considered above. For each possible state \(i\) there will be a set of probabilities \(p_i (j)\) of producing the various possible symbols \(j\). Thus there is an entropy \(H_i\) for each state. The entropy of the source will be defined as the average of these \(H_i\) weighted in accordance with the probability of occurrence of the states in question: </p>
<div class="displaymath" id="a0000000059">
  \begin{align*}  H & = \sum _i P_i H_i \\ & = - \sum _{i,j} P_i p_i (j) \log p_i (j) \,  . \end{align*}
</div>
<p> This is the entropy of the source per symbol of text. If the Markoff process is proceeding at a definite time rate there is also an entropy per second </p>
<div class="displaymath" id="a0000000060">
  \[  H' = \sum _i f_i H_i  \]
</div>
<p> where \(f_i\) is the average frequency (occurrences per second) of state \(i\). Clearly </p>
<div class="displaymath" id="a0000000061">
  \[  H' = mH  \]
</div>
<p> where \(m\) is the average number of symbols produced per second. \(H\) or \(H'\) measures the amount of information generated by the source per symbol or per second. If the logarithmic base is 2, they will represent bits per symbol or per second. </p>
<p>If successive symbols are independent then \(H\) is simply \(- \sum p_i \log p_i\) where \(p_i\) is the probability of symbol \(i\). Suppose in this case we consider a long message of \(N\) symbols. It will contain with high probability about \(p_1 N\) occurrences of the first symbol, \(p_2 N\) occurrences of the second, etc. Hence the probability of this particular message will be roughly </p>
<div class="displaymath" id="a0000000062">
  \[  p= p_1^{p_1 N} p_2^{p_2 N} \dotsm p_n^{p_n N}  \]
</div>
<p> or </p>
<div class="displaymath" id="a0000000063">
  \begin{align*}  \log p & \doteq N \sum _i p_i \log p_i\\ \log p &  \doteq - N H \\ H &  \doteq \frac{\log 1/p}{N}. \end{align*}
</div>
<p> \(H\) is thus approximately the logarithm of the reciprocal probability of a typical long sequence divided by the number of symbols in the sequence. The same result holds for any source. Stated more precisely we have (see Appendix&#160;<a href="ap-3.html">C</a>): </p>
<div class="theorem_thmwrapper theorem-style-plain" id="thm:3">
  <div class="theorem_thmheading">
    <span class="theorem_thmcaption">
    Theorem
    </span>
    <span class="theorem_thmlabel">3</span>
  </div>
  <div class="theorem_thmcontent">
  <p> Given any \(\epsilon {\gt} 0\) and \(\delta {\gt} 0\), we can find an \(N_0\) such that the sequences of any length \(N \ge N_0\) fall into two classes: </p>
<ol class="enumerate">
  <li><p>A set whose total probability is less than \(\epsilon \). </p>
</li>
  <li><p>The remainder, all of whose members have probabilities satisfying the inequality </p>
</li>
</ol>
<div class="displaymath" id="a0000000064">
  \[  \biggl| \frac{\log p^{-1}}{N} - H \biggr| {\lt} \delta .  \]
</div>

  </div>
</div>
<p> In other words we are almost certain to have \(\dfrac {\log p^{-1}}{N}\) very close to \(H\) when \(N\) is large. </p>
<p>A closely related result deals with the number of sequences of various probabilities. Consider again the sequences of length \(N\) and let them be arranged in order of decreasing probability. We define \(n(q)\) to be the number we must take from this set starting with the most probable one in order to accumulate a total probability \(q\) for those taken. </p>
<div class="theorem_thmwrapper theorem-style-plain" id="thm:4">
  <div class="theorem_thmheading">
    <span class="theorem_thmcaption">
    Theorem
    </span>
    <span class="theorem_thmlabel">4</span>
  </div>
  <div class="theorem_thmcontent">
  
<div class="displaymath" id="a0000000065">
  \[  \qopname \relax \@empty {Lim}_{N\to \infty } \frac{\log n(q)}{N} = H  \]
</div>
<p> when \(q\) does not equal \(0\) or \(1\). </p>

  </div>
</div>
<p>We may interpret \(\log n(q)\) as the number of bits required to specify the sequence when we consider only the most probable sequences with a total probability \(q\). Then \(\dfrac {\log n(q)}{N}\) is the number of bits per symbol for the specification. The theorem says that for large \(N\) this will be independent of \(q\) and equal to \(H\). The rate of growth of the logarithm of the number of reasonably probable sequences is given by \(H\), regardless of our interpretation of “reasonably probable.” Due to these results, which are proved in Appendix 3, it is possible for most purposes to treat the long sequences as though there were just \(2^{HN}\) of them, each with a probability \(2^{-HN}\). </p>
<p>The next two theorems show that \(H\) and \(H'\) can be determined by limiting operations directly from the statistics of the message sequences, without reference to the states and transition probabilities between states. </p>
<div class="theorem_thmwrapper theorem-style-plain" id="thm:5">
  <div class="theorem_thmheading">
    <span class="theorem_thmcaption">
    Theorem
    </span>
    <span class="theorem_thmlabel">5</span>
  </div>
  <div class="theorem_thmcontent">
  <p> Let \(p(B_i)\) be the probability of a sequence \(B_i\) of symbols from the source. Let </p>
<div class="displaymath" id="a0000000066">
  \[  G_N = - \frac1N \sum _i p(B_i) \log p(B_i)  \]
</div>
<p> where the sum is over all sequences \(B_i\) containing \(N\) symbols. Then \(G_N\) is a monotonic decreasing function of \(N\) and </p>
<div class="displaymath" id="a0000000067">
  \[  \qopname \relax \@empty {Lim}_{N\to \infty } G_N = H.  \]
</div>

  </div>
</div>
<div class="theorem_thmwrapper theorem-style-plain" id="thm:6">
  <div class="theorem_thmheading">
    <span class="theorem_thmcaption">
    Theorem
    </span>
    <span class="theorem_thmlabel">6</span>
  </div>
  <div class="theorem_thmcontent">
  <p> Let \(p(B_i , S_j)\) be the probability of sequence \(B_i\) followed by symbol \(S_j\) and \(p_{B_i}(S_j) = p(B_i,S_j)/p(B_i)\) be the conditional probability of \(S_j\) after \(B_i\). Let </p>
<div class="displaymath" id="a0000000068">
  \[  F_N = - \sum _{i,j} p(B_i,S_j) \log p_{B_i}(S_j)  \]
</div>
<p> where the sum is over all blocks \(B_i\) of \(N-1\) symbols and over all symbols \(S_j\). Then \(F_N\) is a monotonic decreasing function of \(N\), </p>
<div class="displaymath" id="a0000000069">
  \begin{align*}  F_N & = N G_N - (N-1) G_{N-1}, \\ G_N & = \frac1N \sum _{n=1}^N F_n, \\ F_N & \le G_N, \end{align*}
</div>
<p> and \(\qopname \relax \@empty {Lim}_{N\to \infty } F_N = H\). </p>

  </div>
</div>
<p>These results are derived in Appendix&#160;<a href="ap-3.html">C</a>. They show that a series of approximations to \(H\) can be obtained by considering only the statistical structure of the sequences extending over \(1,2,\dots ,N\) symbols. \(F_N\) is the better approximation. In fact \(F_N\) is the entropy of the \(N^{\text{th}}\) order approximation to the source of the type discussed above. If there are no statistical influences extending over more than \(N\) symbols, that is if the conditional probability of the next symbol knowing the preceding \((N-1)\) is not changed by a knowledge of any before that, then \(F_N = H\). \(F_N\) of course is the conditional entropy of the next symbol when the \((N-1)\) preceding ones are known, while \(G_N\) is the entropy per symbol of blocks of \(N\) symbols. </p>
<p>The ratio of the entropy of a source to the maximum value it could have while still restricted to the same symbols will be called its <em>relative entropy</em>. This is the maximum compression possible when we encode into the same alphabet. One minus the relative entropy is the <em>redundancy</em>. The redundancy of ordinary English, not considering statistical structure over greater distances than about eight letters, is roughly 50%. This means that when we write English half of what we write is determined by the structure of the language and half is chosen freely. The figure 50% was found by several independent methods which all gave results in this neighborhood. One is by calculation of the entropy of the approximations to English. A second method is to delete a certain fraction of the letters from a sample of English text and then let someone attempt to restore them. If they can be restored when 50% are deleted the redundancy must be greater than 50%. A third method depends on certain known results in cryptography. </p>
<p>Two extremes of redundancy in English prose are represented by Basic English and by James Joyce’s book “Finnegans Wake”. The Basic English vocabulary is limited to 850 words and the redundancy is very high. This is reflected in the expansion that occurs when a passage is translated into Basic English. Joyce on the other hand enlarges the vocabulary and is alleged to achieve a compression of semantic content. </p>
<p>The redundancy of a language is related to the existence of crossword puzzles. If the redundancy is zero any sequence of letters is a reasonable text in the language and any two-dimensional array of letters forms a crossword puzzle. If the redundancy is too high the language imposes too many constraints for large crossword puzzles to be possible. A more detailed analysis shows that if we assume the constraints imposed by the language are of a rather chaotic and random nature, large crossword puzzles are just possible when the redundancy is 50%. If the redundancy is 33%, three-dimensional crossword puzzles should be possible, etc. </p>

</div> <!--main-text -->
</div> <!-- content-wrapper -->
</div> <!-- content -->
</div> <!-- wrapper -->

<nav class="prev_up_next">
  <a href="sect0007.html" title="Choice, Uncertainty and Entropy"><svg  class="icon icon-arrow-left "><use xlink:href="symbol-defs.svg#icon-arrow-left"></use></svg>
</a>
  <a href="sect0002.html" title="Discrete Noiseless Systems"><svg  class="icon icon-arrow-up "><use xlink:href="symbol-defs.svg#icon-arrow-up"></use></svg>
</a>
  <a href="sect0009.html" title="Representation of the Encoding and Decoding Operations"><svg  class="icon icon-arrow-right "><use xlink:href="symbol-defs.svg#icon-arrow-right"></use></svg>
</a>
</nav>

<script type="text/javascript" src="js/jquery.min.js"></script>
<script type="text/javascript" src="js/plastex.js"></script>
<script type="text/javascript" src="js/svgxuse.js"></script>
</body>
</html>