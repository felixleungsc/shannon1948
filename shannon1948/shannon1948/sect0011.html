<!DOCTYPE html>
<html lang="en">
<head>
<script>
  MathJax = { 
    tex: {
		    inlineMath: [['\\(','\\)']]
	} }
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<meta name="generator" content="plasTeX" />
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>A Mathematical Theory of Communication: Discussion and Examples</title>
<link rel="next" href="sect0012.html" title="The Discrete Channel with Noise" />
<link rel="prev" href="sect0010.html" title="The Fundamental Theorem for a Noiseless Channel" />
<link rel="up" href="sect0002.html" title="Discrete Noiseless Systems" />
<link rel="stylesheet" href="styles/theme-white.css" />
<link rel="stylesheet" href="styles/amsthm.css" />
</head>

<body>
<header>
<svg  id="toc-toggle" class="icon icon-list-numbered "><use xlink:href="symbol-defs.svg#icon-list-numbered"></use></svg>
<h1 id="doc_title"><a href="index.html">A Mathematical Theory of Communication</a></h1>
</header>

<div class="wrapper">
<nav class="toc">
<ul class="sub-toc-0">
<li class="">
  <a href="sect0001.html"><span class="toc_ref"></span> <span class="toc_entry">Introduction</span></a>
 </li>
<li class=" active">
  <a href="sect0002.html"><span class="toc_ref">1</span> <span class="toc_entry">Discrete Noiseless Systems</span></a>
  <span class="expand-toc">▼</span>
  <ul class="sub-toc-1 active">
     <li class="">
  <a href="sec-1.html"><span class="toc_ref">1</span> <span class="toc_entry">The Discrete Noiseless Channel</span></a>
 </li>
<li class="">
  <a href="sect0003.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Source of Information</span></a>
 </li>
<li class="">
  <a href="sect0004.html"><span class="toc_ref">3</span> <span class="toc_entry">The Series of Approximations to English</span></a>
 </li>
<li class="">
  <a href="sect0005.html"><span class="toc_ref">4</span> <span class="toc_entry">Graphical Representation of a Markoff Process</span></a>
 </li>
<li class="">
  <a href="sect0006.html"><span class="toc_ref">5</span> <span class="toc_entry">Ergodic and Mixed Sources</span></a>
 </li>
<li class="">
  <a href="sect0007.html"><span class="toc_ref">6</span> <span class="toc_entry">Choice, Uncertainty and Entropy</span></a>
 </li>
<li class="">
  <a href="sect0008.html"><span class="toc_ref">7</span> <span class="toc_entry">The Entropy of an Information Source</span></a>
 </li>
<li class="">
  <a href="sect0009.html"><span class="toc_ref">8</span> <span class="toc_entry">Representation of the Encoding and Decoding Operations</span></a>
 </li>
<li class="">
  <a href="sect0010.html"><span class="toc_ref">9</span> <span class="toc_entry">The Fundamental Theorem for a Noiseless Channel</span></a>
 </li>
<li class=" active current">
  <a href="sect0011.html"><span class="toc_ref">10</span> <span class="toc_entry">Discussion and Examples</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0012.html"><span class="toc_ref">2</span> <span class="toc_entry">The Discrete Channel with Noise</span></a>
  <span class="expand-toc">▶</span>
  <ul class="sub-toc-1">
     <li class="">
  <a href="sect0013.html"><span class="toc_ref">11</span> <span class="toc_entry">Representation of a Noisy Discrete Channel</span></a>
 </li>
<li class="">
  <a href="sect0014.html"><span class="toc_ref">12</span> <span class="toc_entry">Equivocation and Channel Capacity</span></a>
 </li>
<li class="">
  <a href="sect0015.html"><span class="toc_ref">13</span> <span class="toc_entry">The Fundamental Theorem for a Discrete Channel with Noise</span></a>
 </li>
<li class="">
  <a href="sect0016.html"><span class="toc_ref">14</span> <span class="toc_entry">Discussion</span></a>
 </li>
<li class="">
  <a href="sect0017.html"><span class="toc_ref">15</span> <span class="toc_entry">Example of a Discrete Channel and its Capacity</span></a>
 </li>
<li class="">
  <a href="sect0018.html"><span class="toc_ref">16</span> <span class="toc_entry">The Channel Capacity in Certain Special Cases</span></a>
 </li>
<li class="">
  <a href="sect0019.html"><span class="toc_ref">17</span> <span class="toc_entry">An Example of Efficient Coding</span></a>
 </li>
<li class="">
  <a href="ap-1.html"><span class="toc_ref">A</span> <span class="toc_entry">The Growth of the Number of Blocks of Symbols with a Finite State Condition</span></a>
 </li>
<li class="">
  <a href="ap-2.html"><span class="toc_ref">B</span> <span class="toc_entry">Derivation of \(H=-\sum p_i\log p_i\)</span></a>
 </li>
<li class="">
  <a href="ap-3.html"><span class="toc_ref">C</span> <span class="toc_entry">Theorems on Ergodic Sources</span></a>
 </li>
<li class="">
  <a href="ap-4.html"><span class="toc_ref">D</span> <span class="toc_entry">Maximizing the Rate for a System of Constraints</span></a>
 </li>

  </ul>
 </li>
<li class="">
  <a href="sect0020.html"><span class="toc_ref">3</span> <span class="toc_entry">Mathematical Preliminaries</span></a>
 </li>
<li class="">
  <a href="sect0021.html"><span class="toc_ref">4</span> <span class="toc_entry">The Continuous Channel</span></a>
 </li>
<li class="">
  <a href="sect0022.html"><span class="toc_ref">5</span> <span class="toc_entry">The Rate for a Continuous Source</span></a>
 </li>
</ul>
</nav>

<div class="content">
<div class="content-wrapper">


<div class="main-text">
<h1 id="a0000000012">10 Discussion and Examples</h1>
<p>In order to obtain the maximum power transfer from a generator to a load, a transformer must in general be introduced so that the generator as seen from the load has the load resistance. The situation here is roughly analogous. The transducer which does the encoding should match the source to the channel in a statistical sense. The source as seen from the channel through the transducer should have the same statistical structure as the source which maximizes the entropy in the channel. The content of Theorem&#160;<a href="sect0010.html#thm:9">9</a> is that, although an exact match is not in general possible, we can approximate it as closely as desired. The ratio of the actual rate of transmission to the capacity \(C\) may be called the efficiency of the coding system. This is of course equal to the ratio of the actual entropy of the channel symbols to the maximum possible entropy. </p>
<p>In general, ideal or nearly ideal encoding requires a long delay in the transmitter and receiver. In the noiseless case which we have been considering, the main function of this delay is to allow reasonably good matching of probabilities to corresponding lengths of sequences. With a good code the logarithm of the reciprocal probability of a long message must be proportional to the duration of the corresponding signal, in fact </p>
<div class="displaymath" id="a0000000083">
  \[  \Bigl| \frac{\log p^{-1}}{T} - C \Bigr|  \]
</div>
<p> must be small for all but a small fraction of the long messages. </p>
<p>If a source can produce only one particular message its entropy is zero, and no channel is required. For example, a computing machine set up to calculate the successive digits of \(\pi \) produces a definite sequence with no chance element. No channel is required to “transmit” this to another point. One could construct a second machine to compute the same sequence at the point. However, this may be impractical. In such a case we can choose to ignore some or all of the statistical knowledge we have of the source. We might consider the digits of \(\pi \) to be a random sequence in that we construct a system capable of sending any sequence of digits. In a similar way we may choose to use some of our statistical knowledge of English in constructing a code, but not all of it. In such a case we consider the source with the maximum entropy subject to the statistical conditions we wish to retain. The entropy of this source determines the channel capacity which is necessary and sufficient. In the \(\pi \) example the only information retained is that all the digits are chosen from the set \(0, 1,\dots , 9\). In the case of English one might wish to use the statistical saving possible due to letter frequencies, but nothing else. The maximum entropy source is then the first approximation to English and its entropy determines the required channel capacity. </p>
<p>As a simple example of some of these results consider a source which produces a sequence of letters chosen from among \(A\), \(B\), \(C\), \(D\) with probabilities \(\frac12\), \(\frac14\), \(\frac18\), \(\frac18\), successive symbols being chosen independently. We have </p>
<div class="displaymath" id="a0000000084">
  \begin{align*}  H & =-\bigl(\tfrac 12\log \tfrac 12 +\tfrac 14\log \tfrac 14+\tfrac 28\log \tfrac 18\bigr) \\ & =\tfrac 74\; \text{bits per symbol}. \end{align*}
</div>
<p> Thus we can approximate a coding system to encode messages from this source into binary digits with an average of \(\frac74\) binary digit per symbol. In this case we can actually achieve the limiting value by the following code (obtained by the method of the second proof of Theorem&#160;<a href="sect0010.html#thm:9">9</a>): </p>
<div class="displaymath" id="a0000000085">
  \[  \begin{array}{c@{\hspace {2em}}r} A &  0 \\ B &  10 \\ C &  110 \\ D &  111 \end{array}  \]
</div>
<p> The average number of binary digits used in encoding a sequence of \(N\) symbols will be </p>
<div class="displaymath" id="a0000000086">
  \[  N \bigl(\tfrac 12 \times 1 + \tfrac 14 \times 2 + \frac28 \times 3 \bigr) = \tfrac 74 N.  \]
</div>
<p> It is easily seen that the binary digits 0, 1 have probabilities \(\frac12\), \(\frac12\) so the \(H\) for the coded sequences is one bit per symbol. Since, on the average, we have \(\frac74\) binary symbols per original letter, the entropies on a time basis are the same. The maximum possible entropy for the original set is \(\log 4=2\), occurring when \(A\), \(B\), \(C\), \(D\) have probabilities \(\frac14\), \(\frac14\), \(\frac14\), \(\frac14\). Hence the relative entropy is \(\frac78\). We can translate the binary sequences into the original set of symbols on a two-to-one basis by the following table: </p>
<div class="displaymath" id="a0000000087">
  \[  \begin{array}{c@{\hspace {2em}}c} 00 &  A’ \\ 01 &  B’ \\ 10 &  C’ \\ 11 &  D’ \end{array}  \]
</div>
<p> This double process then encodes the original message into the same symbols but with an average compression ratio \(\frac78\). </p>
<p>As a second example consider a source which produces a sequence of \(A\)’s and \(B\)’s with probability \(p\) for \(A\) and \(q\) for \(B\). If \(p \ll q\) we have </p>
<div class="displaymath" id="a0000000088">
  \begin{align*}  H & = - \log p^p (1-p)^{1-p} \\ & = - p \log p(1-p)^{(1-p)/p} \\ & \doteq p \log \frac ep. \end{align*}
</div>
<p> In such a case one can construct a fairly good coding of the message on a 0, 1 channel by sending a special sequence, say 0000, for the infrequent symbol \(A\) and then a sequence indicating the <em>number</em> of \(B\)’s following it. This could be indicated by the binary representation with all numbers containing the special sequence deleted. All numbers up to 16 are represented as usual; 16 is represented by the next binary number after 16 which does not contain four zeros, namely \(17 = 10001\), etc. </p>
<p>It can be shown that as \(p \to 0\) the coding approaches ideal provided the length of the special sequence is properly adjusted. </p>

</div> <!--main-text -->
</div> <!-- content-wrapper -->
</div> <!-- content -->
</div> <!-- wrapper -->

<nav class="prev_up_next">
  <a href="sect0010.html" title="The Fundamental Theorem for a Noiseless Channel"><svg  class="icon icon-arrow-left "><use xlink:href="symbol-defs.svg#icon-arrow-left"></use></svg>
</a>
  <a href="sect0002.html" title="Discrete Noiseless Systems"><svg  class="icon icon-arrow-up "><use xlink:href="symbol-defs.svg#icon-arrow-up"></use></svg>
</a>
  <a href="sect0012.html" title="The Discrete Channel with Noise"><svg  class="icon icon-arrow-right "><use xlink:href="symbol-defs.svg#icon-arrow-right"></use></svg>
</a>
</nav>

<script type="text/javascript" src="js/jquery.min.js"></script>
<script type="text/javascript" src="js/plastex.js"></script>
<script type="text/javascript" src="js/svgxuse.js"></script>
</body>
</html>